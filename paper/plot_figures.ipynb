{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd95df49",
   "metadata": {},
   "source": [
    "# Plot figures in the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ad2c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import torch\n",
    "from fastabx import Dataset\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "from scipy.stats import linregress, pearsonr\n",
    "from sklearn.manifold import TSNE\n",
    "from torch import nn\n",
    "\n",
    "from spidr.config import DinoSRConfig, MaskingConfig, OptimizerConfig, SpidRConfig\n",
    "from spidr.data.masks import MaskGenerator\n",
    "from spidr.models.dinosr import ema_scheduler\n",
    "from spidr.models.metrics import proba_phone_code\n",
    "from spidr.models.spidr import exp_ema_scheduler\n",
    "from spidr.optimizer import build_optimizer\n",
    "\n",
    "\n",
    "def mpl_palette(name: str, n_colors: int) -> list[tuple[float, float, float]]:\n",
    "    \"\"\"Adapted from seaborn.mpl_palette.\"\"\"\n",
    "    # fmt: off\n",
    "    mpl_qual_pals = {\n",
    "        \"tab10\": 10, \"tab20\": 20, \"tab20b\": 20, \"tab20c\": 20, \"Set1\": 9, \"Set2\": 8, \"Set3\": 12,\n",
    "        \"Accent\": 8, \"Paired\": 12, \"Pastel1\": 9, \"Pastel2\": 8, \"Dark2\": 8,\n",
    "    }\n",
    "    # fmt: on\n",
    "    bins = (\n",
    "        np.linspace(0, 1, mpl_qual_pals[name])[:n_colors]\n",
    "        if name in mpl_qual_pals\n",
    "        else np.linspace(0, 1, int(n_colors) + 2)[1:-1]\n",
    "    )\n",
    "    return list(map(tuple, plt.get_cmap(name)(bins)[:, :3]))\n",
    "\n",
    "\n",
    "assets, results, figures = Path(\"./assets\"), Path(\"./results\"), Path(\"./figures\")\n",
    "plt.style.use(assets / \"paper.mplstyle\")\n",
    "figures.mkdir(exist_ok=True)\n",
    "\n",
    "TEXTWIDTH = 6.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a9a160",
   "metadata": {},
   "source": [
    "## 3 - Method\n",
    "\n",
    "Figure 2: Codebook and prediction perplexities during training for SpidR and DinoSR on LibriSpeech dev-clean, with $K = 8$ codebooks. For each layer $k$, the codebook perplexity is computed over each batch with $\\bm{p} = \\bm{y}^k$ and then averaged across the dataset. The prediction perplexity uses $\\bm{p} = \\tilde{\\bm{y}}^k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f5f163",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ppl(subset: str) -> tuple[pl.DataFrame, pl.DataFrame, pl.DataFrame, pl.DataFrame]:\n",
    "    return (\n",
    "        pl.read_ndjson(results / \"logs\" / f\"{model}-{subset}.jsonl.gz\")\n",
    "        .filter(pl.col(\"rank\") == 0)\n",
    "        .with_columns((pl.col(f\"{metric}_{i}\") * pl.col(f\"num_frames_{i}\")).alias(f\"{metric}_{i}\") for i in range(8))\n",
    "        .group_by(\"step\")\n",
    "        .agg([pl.col(f\"{metric}_{i}\").sum() for i in range(8)] + [pl.col(f\"num_frames_{i}\").sum() for i in range(8)])\n",
    "        .with_columns([(pl.col(f\"{metric}_{i}\") / pl.col(f\"num_frames_{i}\")).alias(f\"{metric}_{i}\") for i in range(8)])\n",
    "        .select([\"step\"] + [f\"{metric}_{i}\" for i in range(8)])\n",
    "        .sort(\"step\")\n",
    "        for model in [\"dinosr\", \"spidr\"]\n",
    "        for metric in [\"target_ppl\", \"pred_ppl\"]\n",
    "    )\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(TEXTWIDTH, 3), nrows=2, ncols=4, sharex=True, sharey=True)\n",
    "fig.set_layout_engine(\"tight\", pad=0, w_pad=0, h_pad=0.8, rect=(0, 0.2, 1, 1))\n",
    "subset = \"dev-clean\"\n",
    "dinosr_target, dinosr_pred, spidr_target, spidr_pred = get_ppl(subset)\n",
    "for k in range(8):\n",
    "    i, j = divmod(k, 4)\n",
    "    ax[i, j].plot(spidr_target[\"step\"], spidr_target[f\"target_ppl_{k}\"], color=\"C0\", lw=1)\n",
    "    ax[i, j].plot(dinosr_target[\"step\"], dinosr_target[f\"target_ppl_{k}\"], color=\"C3\", lw=1)\n",
    "    ax[i, j].plot(spidr_pred[\"step\"], spidr_pred[f\"pred_ppl_{k}\"], linestyle=\"--\", color=\"C0\", lw=1)\n",
    "    ax[i, j].plot(dinosr_pred[\"step\"], dinosr_pred[f\"pred_ppl_{k}\"], linestyle=\"--\", color=\"C3\", lw=1)\n",
    "    ax[i, j].set_title(f\"Layer {5 + k}\", fontsize=10)\n",
    "    if j == 0:\n",
    "        ax[i, j].set_ylabel(\"Perplexity\")\n",
    "    ymin = min(\n",
    "        spidr_target[1:, f\"target_ppl_{k}\"].min(),\n",
    "        dinosr_target[1:, f\"target_ppl_{k}\"].min(),\n",
    "        spidr_pred[1:, f\"pred_ppl_{k}\"].min(),\n",
    "        dinosr_pred[1:, f\"pred_ppl_{k}\"].min(),\n",
    "    )\n",
    "    ymax = max(\n",
    "        spidr_target[1:, f\"target_ppl_{k}\"].max(),\n",
    "        dinosr_target[1:, f\"target_ppl_{k}\"].max(),\n",
    "        spidr_pred[1:, f\"pred_ppl_{k}\"].max(),\n",
    "        dinosr_pred[1:, f\"pred_ppl_{k}\"].max(),\n",
    "    )\n",
    "    ax[i, j].set_ylim(ymin * 0.95, ymax * 1.05)\n",
    "    ax[i, j].set_xticks([0, 200_000, 400_000])\n",
    "    ax[i, j].set_xticklabels([\"0\", \"200k\", \"400k\"])\n",
    "    ax[i, j].xaxis.set_tick_params(labelsize=7)\n",
    "    ax[i, j].yaxis.set_tick_params(labelsize=7)\n",
    "    ax[i, j].grid(linestyle=\"dotted\")\n",
    "fig.supxlabel(\"Training steps\", y=0.18, fontsize=10)\n",
    "leg = ax[1, 0].legend(\n",
    "    [Line2D([], [], color=\"C0\", lw=2), Line2D([], [], color=\"C3\", lw=2)],\n",
    "    [\"SpidR\", \"DinoSR\"],\n",
    "    bbox_to_anchor=(0.8, -0.49),\n",
    "    loc=\"upper center\",\n",
    "    fontsize=10,\n",
    "    title=\"Model\",\n",
    "    ncols=2,\n",
    ")\n",
    "leg.set_in_layout(False)\n",
    "leg2 = ax[1, 2].legend(\n",
    "    [Line2D([], [], color=\"black\", lw=2), Line2D([], [], color=\"black\", linestyle=\"--\", lw=2)],\n",
    "    [\"From codebook\", \"From prediction\"],\n",
    "    bbox_to_anchor=(0.9, -0.49),\n",
    "    loc=\"upper center\",\n",
    "    fontsize=10,\n",
    "    title=\"Perplexity\",\n",
    "    ncols=2,\n",
    ")\n",
    "leg2.set_in_layout(False)\n",
    "plt.savefig(figures / f\"perplexity-{subset}.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5724a5a3",
   "metadata": {},
   "source": [
    "## 4.4 - Evaluation of downstream spoken language modeling\n",
    "\n",
    "Figure 3: Data scaling results for a 125M parameters OPT model trained on Libri-Light, with different discrete units encoders. Zero-shot accuracy in %, chance level 50%. The speech encoders have V = 256 units. The log-likelihoods are normalized by the number of tokens, except for WUGGY with text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075a452b",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {\"swuggy_all\": \"WUGGY all\", \"sblimp\": \"BLIMP\", \"tsc\": \"tSC\"}\n",
    "styles = {\n",
    "    \"spidr_codebook\": {\"label\": \"SpidR (Codebooks)\", \"linestyle\": \"-\"},\n",
    "    \"spidr_kmeans\": {\"label\": \"SpidR (K-means)\", \"linestyle\": \"--\"},\n",
    "    \"hubert_kmeans\": {\"label\": \"HuBERT (K-means)\", \"linestyle\": \"-.\"},\n",
    "    \"text\": {\"label\": \"Text (BPE)\", \"color\": \"#484848\", \"linestyle\": \":\"},\n",
    "}\n",
    "layers = {\"spidr_codebook\": 6, \"spidr_kmeans\": 6, \"hubert_kmeans\": 11}\n",
    "\n",
    "df = pl.read_csv(results / \"spoken-lm.csv\")\n",
    "fig, ax = plt.subplots(nrows=1, ncols=len(metrics), figsize=(TEXTWIDTH, 2), sharex=True)\n",
    "fig.set_layout_engine(\"tight\", pad=0.1, w_pad=1, rect=(0, 0.22, 1, 1))\n",
    "for k, metric in enumerate(metrics):\n",
    "    for model, style in styles.items():\n",
    "        select_layer = (pl.col(\"layer\") == layers[model]) if model != \"text\" else pl.col(\"layer\").is_null()\n",
    "        subdf = df.filter(pl.col(\"metric\") == metric, pl.col(\"model\") == model, select_layer).sort(\"hours\")\n",
    "        ax[k].plot(subdf[\"hours\"], subdf[\"score\"], lw=1.5, marker=\"o\", markersize=4, **style)\n",
    "    ax[k].set_xscale(\"log\")\n",
    "    ax[k].xaxis.set_major_formatter(ScalarFormatter())\n",
    "    ax[k].minorticks_off()\n",
    "    ax[k].set_xticks([600, 6000, 60_000])\n",
    "    ax[k].grid(linestyle=\"dotted\")\n",
    "    ax[k].xaxis.set_tick_params(labelsize=7)\n",
    "    ax[k].yaxis.set_tick_params(labelsize=7)\n",
    "    ax[k].set_title(metrics[metric], fontsize=10)\n",
    "ax[0].set_ylabel(r\"Accuracy (\\%)\", fontsize=10)\n",
    "fig.supxlabel(\"Training hours\", y=0.2, fontsize=10)\n",
    "leg = ax[1].legend(bbox_to_anchor=(0.47, -0.4), loc=\"upper center\", ncols=len(styles), fontsize=9)\n",
    "leg.set_in_layout(False)\n",
    "plt.savefig(figures / \"scaling.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47b6c2d",
   "metadata": {},
   "source": [
    "## 4.5 - Codebase and pretraining time\n",
    "\n",
    "Figure 4: Approximate pretraining time for various hardware configurations with constant total batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd024352",
   "metadata": {},
   "outputs": [],
   "source": [
    "styles = {\n",
    "    False: {\"fill\": True, \"facecolor\": \"gray\", \"edgecolor\": \"black\"},\n",
    "    True: {\"fill\": False, \"hatch\": \"///\", \"edgecolor\": \"black\"},\n",
    "}\n",
    "width = 0.3\n",
    "\n",
    "df = pl.read_csv(results / \"speed.csv\")\n",
    "fig, ax = plt.subplots(figsize=(TEXTWIDTH * 0.35, 2.15), nrows=2, sharey=True, sharex=True)\n",
    "fig.set_layout_engine(\"tight\", rect=(0.1, 0.06, 0.85, 1), pad=0.05, h_pad=0.5)\n",
    "x = np.arange(len(df[\"n\"].unique()))\n",
    "for i, gpu in enumerate([\"a100\", \"h100\"]):\n",
    "    for multiplier, with_compile in enumerate([False, True]):\n",
    "        subdf = df.filter((pl.col(\"gpu\") == gpu) & (pl.col(\"compile\") == with_compile)).sort(\"n\", descending=False)\n",
    "        offset = width * multiplier\n",
    "        rects = ax[i].bar(\n",
    "            x + offset,\n",
    "            subdf[\"hours\"],\n",
    "            width,\n",
    "            label={True: \"W/ compile\", False: \"W/o compile\"}[with_compile],\n",
    "            **styles[with_compile],\n",
    "        )\n",
    "        ax[i].bar_label(rects, padding=3, fontsize=9)\n",
    "    ax[i].set_yticks([])\n",
    "    ax[i].set_xticks(x + width / 2, sorted(df[\"n\"].unique()), fontsize=9)\n",
    "    ax[i].set_title(f\"{gpu.upper()}\", y=0.7 if i == 1 else None, fontsize=10)\n",
    "handles, labels = ax[0].get_legend_handles_labels()\n",
    "leg = fig.legend(\n",
    "    handles,\n",
    "    labels,\n",
    "    loc=\"center right\",\n",
    "    bbox_to_anchor=(1, 0.45),\n",
    "    fontsize=6,\n",
    ")\n",
    "leg.set_in_layout(False)\n",
    "text = fig.supylabel(\"Pretraining time (hours)\", fontsize=9)\n",
    "text.set_in_layout(False)\n",
    "text = fig.supxlabel(\"Number of GPUs\", fontsize=9)\n",
    "text.set_in_layout(False)\n",
    "plt.savefig(figures / \"speed.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbf25c5",
   "metadata": {},
   "source": [
    "## A.1 - SpidR pretraining\n",
    "\n",
    "Figure 5: Learning rate schedule and EMA decay schedule of the teacher for DinoSR and SpidR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cc9159",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Linear(1, 1)\n",
    "cfg = OptimizerConfig()\n",
    "opt, _, scheduler = build_optimizer(model, cfg)\n",
    "d, s, ema_s, ema_d, lrs = DinoSRConfig(), SpidRConfig(), [], [], []\n",
    "for i in range(400_000):  # Naive loop\n",
    "    opt.step()\n",
    "    lrs.append(scheduler.get_last_lr()[0])\n",
    "    scheduler.step()\n",
    "    ema_s.append(exp_ema_scheduler(i, s.ema_start_decay, s.ema_timescale, s.ema_threshold))\n",
    "    ema_d.append(ema_scheduler(i, d.ema_start_decay, d.ema_final_decay, d.ema_final_step, d.freeze_step))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(3, 1.5))\n",
    "fig.set_layout_engine(\"tight\", pad=0.1)\n",
    "ax.plot(lrs, color=\"k\", lw=2)\n",
    "ax.set_xlabel(\"Training steps\")\n",
    "ax.set_ylabel(\"Learning rate\")\n",
    "ax.ticklabel_format(style=\"scientific\", axis=\"y\", scilimits=(0, 0))\n",
    "ax.set_yticks([0, 1e-4, 2e-4, 3e-4, 4e-4, 5e-4])\n",
    "ax.set_xticks([0, 200_000, 400_000])\n",
    "ax.grid(linestyle=\"dotted\")\n",
    "ax.set_xticklabels([\"0\", \"200k\", \"400k\"])\n",
    "plt.savefig(figures / \"lr-scheduler.pdf\")\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(3, 1.5))\n",
    "fig.set_layout_engine(\"tight\", pad=0.1)\n",
    "ax.plot(ema_s, color=\"C0\", lw=2, zorder=3, label=\"SpidR\")\n",
    "ax.plot(ema_d, color=\"C3\", lw=2, zorder=2, label=\"DinoSR\", linestyle=\"--\")\n",
    "ax.set_xlabel(\"Training steps\")\n",
    "ax.set_ylabel(r\"$\\beta_t$\")\n",
    "ax.set_xticks([0, 200_000, 400_000])\n",
    "ax.set_xticklabels([\"0\", \"200k\", \"400k\"])\n",
    "ax.legend(loc=\"lower right\")\n",
    "ax.grid(linestyle=\"dotted\")\n",
    "plt.savefig(figures / \"ema-scheduler.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d4e116",
   "metadata": {},
   "source": [
    "## A.2 - Masking procedure\n",
    "\n",
    "Figure 6: Mask and unmasked frames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e992f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_length(length: torch.Tensor) -> torch.Tensor:\n",
    "    conv_layer_config = [(10, 5)] + [(3, 2)] * 4 + [(2, 2)] * 2\n",
    "    for kernel_size, stride in conv_layer_config:\n",
    "        length = torch.div(length - kernel_size, stride, rounding_mode=\"floor\") + 1\n",
    "        length = torch.max(torch.zeros_like(length), length)\n",
    "    return length\n",
    "\n",
    "\n",
    "def get_mask(sample_size: int, dim: int = 200) -> torch.Tensor:\n",
    "    torch.manual_seed(1)\n",
    "    gen = MaskGenerator(MaskingConfig())\n",
    "    max_len = conv_length(torch.tensor(sample_size)).item()\n",
    "    x = torch.zeros((1, max_len), dtype=torch.bool)\n",
    "    return (~gen(x)[0]).float().unsqueeze(-1).expand(dim, -1, 3)\n",
    "\n",
    "\n",
    "mask = get_mask(216000)\n",
    "color = torch.tensor([225, 225, 225], dtype=torch.float32) / 255.0\n",
    "x = torch.ones_like(mask) * color.view(1, 1, 3)\n",
    "\n",
    "plt.figure(figsize=(10, 2))\n",
    "plt.imshow(x)\n",
    "plt.axis(\"off\")\n",
    "plt.savefig(figures / \"x.pdf\", bbox_inches=\"tight\", pad_inches=0)\n",
    "plt.show()\n",
    "plt.figure(figsize=(10, 2))\n",
    "plt.imshow(x * mask)\n",
    "plt.axis(\"off\")\n",
    "plt.savefig(figures / \"x-tilde.pdf\", bbox_inches=\"tight\", pad_inches=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da68e905",
   "metadata": {},
   "source": [
    "## B.1 - Discriminability of continuous embeddings\n",
    "\n",
    "Figure 7: ABX and MAP (in %, chancel level 50% for ABX) by layer for SpidR, DinoSR and HuBERT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a162bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mapping = {\"hubert\": \"HuBERT\", \"spidr\": \"SpidR\", \"dinosr\": \"DinoSR\", \"dinosr_ours\": \"DinoSR (ours)\"}\n",
    "\n",
    "abx = (\n",
    "    pl.read_csv(results / \"abx-continuous.csv\")\n",
    "    .filter(pl.col(\"model\").is_in(model_mapping))\n",
    "    .with_columns(pl.lit(\"abx\").alias(\"metric\"), pl.col(\"model\").replace_strict(model_mapping))\n",
    "    .group_by([\"model\", \"layer\"])\n",
    "    .agg(pl.col(\"score\").mean())\n",
    ")\n",
    "speech_map = (\n",
    "    pl.read_csv(results / \"map-continuous.csv\")\n",
    "    .filter(pl.col(\"model\").is_in(model_mapping))\n",
    "    .with_columns(pl.lit(\"map\").alias(\"metric\"), pl.col(\"model\").replace_strict(model_mapping))\n",
    "    .group_by([\"model\", \"layer\"])\n",
    "    .agg(pl.col(\"score\").mean())\n",
    ")\n",
    "\n",
    "styles = {\n",
    "    \"SpidR\": {\"color\": \"C0\"},\n",
    "    \"HuBERT\": {\"color\": \"C2\", \"linestyle\": \"-.\"},\n",
    "    \"DinoSR\": {\"color\": \"C3\", \"linestyle\": \"--\"},\n",
    "    \"DinoSR (ours)\": {\"color\": \"C3\", \"linestyle\": \"--\", \"alpha\": 0.3, \"markeredgecolor\": \"none\"},\n",
    "}\n",
    "common = {\"marker\": \"o\", \"markersize\": 4, \"linewidth\": 1}\n",
    "models = [\"SpidR\", \"DinoSR\", \"DinoSR (ours)\", \"HuBERT\"]\n",
    "\n",
    "plt.style.use(\"mpoli\")\n",
    "fig, ax = plt.subplots(figsize=(6.5, 2), ncols=2, sharex=True, sharey=False)\n",
    "fig.set_layout_engine(\"tight\", pad=1, w_pad=1, rect=(0, 0.1, 1, 1))\n",
    "for i, model in enumerate(models):\n",
    "    abx_df = abx.filter(pl.col(\"model\") == model).sort(\"layer\")\n",
    "    map_df = speech_map.filter(pl.col(\"model\") == model).sort(\"layer\")\n",
    "    ax[0].plot(abx_df[\"layer\"], abx_df[\"score\"], label=model, zorder=len(models) + 1 - i, **common, **styles[model])\n",
    "    ax[1].plot(map_df[\"layer\"], map_df[\"score\"], label=model, zorder=len(models) + 1 - i, **common, **styles[model])\n",
    "ax[0].grid(linestyle=\"dotted\", alpha=0.4)\n",
    "ax[1].grid(linestyle=\"dotted\", alpha=0.4)\n",
    "ax[0].set_xticks(list(range(1, 13)))\n",
    "ax[0].set_yticks([4, 6, 8, 10, 12, 14])\n",
    "ax[0].set_ylim(4 * 0.9, 14 * 1.1)\n",
    "ax[0].set_xlabel(\"Layer\")\n",
    "ax[1].set_xlabel(\"Layer\")\n",
    "ax[0].set_ylabel(r\"ABX error rate (\\%)\")\n",
    "ax[1].set_ylabel(r\"MAP (\\%)\")\n",
    "leg = ax[0].legend(bbox_to_anchor=(1.1, -0.38), loc=\"upper center\", ncols=4, fontsize=9)\n",
    "leg.set_in_layout(False)\n",
    "plt.savefig(figures / \"continuous-by-layer.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d358370",
   "metadata": {},
   "source": [
    "## B.2 - Embeddings visualization\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "To reproduce the t-SNE figures from the paper, first extract the features from SpidR layer 6 with:\n",
    "\n",
    "```python extract_features.py /path/to/manifests/dev-clean.csv ./features```\n",
    "where \"dev-clean.csv\" is your manifest file to LibriSpeech dev-clean.\n",
    "\n",
    "Then execute the following cells after changing the variable `librispeech` to the actual path of your LibriSpeech copy.\n",
    "</div>\n",
    "\n",
    "Figure 8: t-SNE visualization of phone embeddings from SpidR layer 6 on LibriSpeech dev-clean. Embeddings are colored by phone class and by speaker gender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8ee381",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_data(dataset: Dataset, *, n: int, seed: int) -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    partitions = {\n",
    "        key: value.sample(seed=seed, n=n) if len(value) >= n else None\n",
    "        for key, value in dataset.labels.with_row_index().partition_by(\"#phone\", \"speaker\", as_dict=True).items()\n",
    "    }\n",
    "    phones, speakers, embeddings = [], [], []\n",
    "    for (phone, speaker), samples in partitions.items():\n",
    "        if samples is None:\n",
    "            continue\n",
    "        phones += [phone] * len(samples)\n",
    "        speakers += [speaker] * len(samples)\n",
    "        embeddings += [dataset.accessor[i].mean(dim=0).numpy() for i in samples[\"index\"]]\n",
    "    return np.array(embeddings), np.array(phones), np.array(speakers)\n",
    "\n",
    "\n",
    "def read_gender(speaker_file: str, subset: str) -> dict[str, str]:\n",
    "    data = {}\n",
    "    with Path(speaker_file).open() as f:\n",
    "        lines = f.readlines()[12:]\n",
    "    for line in lines:\n",
    "        x, y, z, *_ = line.split(\" | \")\n",
    "        if z.strip() == subset:\n",
    "            data[x.strip()] = y.strip()\n",
    "    return data\n",
    "\n",
    "\n",
    "features, librispeech = Path(\"./features\"), Path(\"../../LibriSpeech\")  # To adapt to your setup\n",
    "\n",
    "sonority_to_phones = json.loads((assets / \"sonority_to_arpabet.json\").read_text())\n",
    "phone_to_sonority = {phone: sonority for sonority, phones in sonority_to_phones.items() for phone in phones}\n",
    "arpabet_to_tipa = json.loads((assets / \"arpabet_to_tipa.json\").read_text())\n",
    "palette = mpl_palette(\"magma\", len(sonority_to_phones))\n",
    "\n",
    "dataset = Dataset.from_item(assets / \"phoneme-dev-clean.item\", features, 50)\n",
    "embeddings, phones, speakers = build_data(dataset, n=10, seed=0)\n",
    "tsne = TSNE(n_components=2, random_state=0, n_jobs=-1).fit_transform(embeddings)\n",
    "df = pl.DataFrame({\"x\": tsne[:, 0], \"y\": tsne[:, 1], \"phone\": phones, \"speaker\": speakers}).with_columns(\n",
    "    pl.col(\"phone\").replace_strict(phone_to_sonority).alias(\"sonority\"),\n",
    "    pl.col(\"speaker\").replace_strict(read_gender(librispeech / \"SPEAKERS.txt\", \"dev-clean\")).alias(\"gender\"),\n",
    ")\n",
    "common = {\"s\": 5, \"alpha\": 0.4, \"rasterized\": True, \"edgecolors\": \"white\", \"linewidth\": 0.2}\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(TEXTWIDTH, TEXTWIDTH / 2), ncols=2, sharex=True, sharey=True)\n",
    "fig.set_layout_engine(\"tight\", pad=0, rect=(0, 0.18, 1, 1))\n",
    "for i, sonority in enumerate(sonority_to_phones):\n",
    "    subdf = df.filter(pl.col(\"sonority\") == sonority)\n",
    "    ax[0].scatter(subdf[\"x\"], subdf[\"y\"], label=sonority.capitalize(), color=palette[i], **common)\n",
    "ax[0].axis(\"off\")\n",
    "ax[0].set_aspect(\"equal\")\n",
    "legend = ax[0].legend(\n",
    "    title=\"Phone class\",\n",
    "    fontsize=9,\n",
    "    ncols=3,\n",
    "    bbox_to_anchor=(0.5, 0.05),\n",
    "    loc=\"upper center\",\n",
    "    markerscale=2,\n",
    ")\n",
    "for lh in legend.legend_handles:\n",
    "    lh.set_alpha(1)\n",
    "legend.set_in_layout(False)\n",
    "for gender, name in zip([\"F\", \"M\"], [\"Female\", \"Male\"], strict=False):\n",
    "    subdf = df.filter(pl.col(\"gender\") == gender)\n",
    "    ax[1].scatter(subdf[\"x\"], subdf[\"y\"], label=name, **common)\n",
    "ax[1].axis(\"off\")\n",
    "ax[1].set_aspect(\"equal\")\n",
    "legend = ax[1].legend(\n",
    "    title=\"Gender\",\n",
    "    fontsize=9,\n",
    "    ncols=2,\n",
    "    bbox_to_anchor=(0.5, 0.05),\n",
    "    loc=\"upper center\",\n",
    "    markerscale=2,\n",
    ")\n",
    "for lh in legend.legend_handles:\n",
    "    lh.set_alpha(1)\n",
    "legend.set_in_layout(False)\n",
    "plt.savefig(figures / \"tsne.pdf\", dpi=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27feea3c",
   "metadata": {},
   "source": [
    "Figure 9: t-SNE visualization of phone embeddings from SpidR layer 6 on LibriSpeech dev-clean, colored by individual phones within each phone class. Embeddings from other classes are shown in gray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc9860f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(TEXTWIDTH, TEXTWIDTH / 2), nrows=2, ncols=3)\n",
    "fig.set_layout_engine(\"tight\", pad=0.05, h_pad=0, w_pad=4, rect=(0, 0, 1, 1))\n",
    "for k, sonority in enumerate(sonority_to_phones):\n",
    "    i, j = divmod(k, 3)\n",
    "    mult = 1 if sonority == \"vowel\" else 2\n",
    "    palette = mpl_palette(\"tab20\", len(sonority_to_phones[sonority]) * mult)\n",
    "    colors = {phone: palette[i * mult] for i, phone in enumerate(sonority_to_phones[sonority])} | {\n",
    "        phone: \"gray\" for phone in phone_to_sonority if phone not in sonority_to_phones[sonority]\n",
    "    }\n",
    "    for phone in phone_to_sonority:\n",
    "        subdf = df.filter(pl.col(\"phone\") == phone)\n",
    "        ax[i, j].scatter(\n",
    "            subdf[\"x\"],\n",
    "            subdf[\"y\"],\n",
    "            s=1,\n",
    "            label=f\"[{arpabet_to_tipa[phone]}]\" if phone in sonority_to_phones[sonority] else None,\n",
    "            color=colors[phone],\n",
    "            alpha=0.8 if phone in sonority_to_phones[sonority] else 0.02,\n",
    "            edgecolors=\"none\",\n",
    "            rasterized=True,\n",
    "            zorder=3 if phone in sonority_to_phones[sonority] else 1,\n",
    "        )\n",
    "    legend = ax[i, j].legend(\n",
    "        loc=\"upper right\",\n",
    "        bbox_to_anchor=(0.06, 1),\n",
    "        ncols=2 if sonority == \"vowel\" else 1,\n",
    "        columnspacing=0.1,\n",
    "        markerscale=2.5,\n",
    "        fontsize=5,\n",
    "        handletextpad=0.05,\n",
    "    )\n",
    "    for lh in legend.legend_handles:\n",
    "        lh.set_alpha(1)\n",
    "    legend.set_in_layout(False)\n",
    "    ax[i, j].set_title(sonority.capitalize(), fontsize=9)\n",
    "    ax[i, j].set_axis_off()\n",
    "    ax[i, j].set_aspect(\"equal\")\n",
    "plt.savefig(figures / \"tsne-sonority.pdf\", dpi=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773ff13f",
   "metadata": {},
   "source": [
    "Figure 10: P(phone | code) visualization for SpidR layer 6 using either codebook predictions (left) or K-means quantization (right), on LibriSpeech dev-clean and dev-other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d64a7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for method in [\"codebooks\", \"kmeans\"]:\n",
    "    df = pl.read_ndjson(assets / \"phoneme-dev-clean-and-other.jsonl.gz\").join(\n",
    "        pl.read_ndjson(results / f\"units-{method}-spidr-l6-dev-clean-and-other.jsonl.gz\"), on=\"name\"\n",
    "    )\n",
    "    data = {name: (phones, codes) for name, phones, codes in df.iter_rows()}\n",
    "    proba, phone_order, _ = proba_phone_code(data, num_units=256, num_phones=40, only_active=True)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(3, 3))\n",
    "    fig.set_layout_engine(\"tight\", pad=0)\n",
    "    ax.imshow(proba, cmap=\"Blues\", aspect=\"auto\", interpolation=\"none\")\n",
    "    ax.spines[[\"top\", \"right\", \"left\", \"bottom\"]].set_visible(False)\n",
    "    ax.set_yticks(range(len(phone_order)))\n",
    "    ax.set_yticklabels(phone_order, fontsize=5)\n",
    "    ax.tick_params(axis=\"x\", which=\"both\", bottom=False, top=False, labelbottom=False)\n",
    "    plt.savefig(figures / f\"proba-phone-code-{method}.pdf\", bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c376715b",
   "metadata": {},
   "source": [
    "## B.3 - Layer-wise analysis\n",
    "\n",
    "Fgiure 11: ABX (in %, chance level 50%) and PNMI by layer on discrete units from SpidR using codebook predictions or K-means, and from HuBERT using K-means, with V = 256 units. ABX scores averaged across subsets and speaker conditions, and PNMI computed on LibriSpeech dev-clean and dev-other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297129a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "styles = {\n",
    "    \"SpidR (Codebooks)\": {\"color\": \"C0\"},\n",
    "    \"SpidR (K-means)\": {\"color\": \"C1\", \"linestyle\": \"--\"},\n",
    "    \"HuBERT (K-means)\": {\"color\": \"C2\", \"linestyle\": \"-.\"},\n",
    "}\n",
    "common = {\"marker\": \"o\", \"markersize\": 4, \"linewidth\": 1}\n",
    "models = {\"spidr_codebooks\": \"SpidR (Codebooks)\", \"spidr\": \"SpidR (K-means)\", \"hubert\": \"HuBERT (K-means)\"}\n",
    "\n",
    "pnmi = pl.read_csv(results / \"units-quality.csv\")\n",
    "abx = pl.read_csv(results / \"abx-discrete.csv\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(TEXTWIDTH, 2), ncols=2, sharex=True, sharey=False)\n",
    "fig.set_layout_engine(\"tight\", h_pad=1, w_pad=1, rect=(0, 0.09, 1, 1))\n",
    "for i, (name, model) in enumerate(models.items()):\n",
    "    abx_df = abx.filter(pl.col(\"model\") == name).sort(\"layer\").group_by(\"layer\").agg(pl.col(\"score\").mean())\n",
    "    pnmi_df = pnmi.filter(pl.col(\"model\") == name, pl.col(\"metric\") == \"pnmi\").sort(\"layer\")\n",
    "    ax[0].plot(abx_df[\"layer\"], abx_df[\"score\"], label=model, zorder=len(models) + 1 - i, **common, **styles[model])\n",
    "    ax[1].plot(pnmi_df[\"layer\"], pnmi_df[\"score\"], label=model, zorder=len(models) + 1 - i, **common, **styles[model])\n",
    "ax[0].grid(linestyle=\"dotted\", alpha=0.4)\n",
    "ax[1].grid(linestyle=\"dotted\", alpha=0.4)\n",
    "ax[0].set_xticks(list(range(5, 13)))\n",
    "ax[0].set_yticks([4, 6, 8, 10, 12, 14])\n",
    "ax[0].set_ylim(4 * 0.9, 14 * 1.1)\n",
    "ax[0].set_xlabel(\"Layer\")\n",
    "ax[1].set_xlabel(\"Layer\")\n",
    "ax[0].set_ylabel(r\"ABX error rate (\\%)\")\n",
    "ax[1].set_ylabel(r\"PNMI\")\n",
    "leg = ax[1].legend(bbox_to_anchor=(-0.1, -0.38), loc=\"upper center\", ncols=4, fontsize=9)\n",
    "leg.set_in_layout(False)\n",
    "plt.savefig(figures / \"discrete-by-layer.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b59e618",
   "metadata": {},
   "source": [
    "Figure 12: Zero-shot spoken language modeling from each layer of HuBERT and SpidR (in %, chance level 50%), with units from codebook predictions or from K-means quantization, with V = 256 units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178c6bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mapping = {\n",
    "    \"spidr_codebooks\": (\"SpidR (Codebooks)\", {\"color\": \"C0\"}),\n",
    "    \"spidr_kmeans\": (\"SpidR (K-means)\", {\"color\": \"C1\", \"linestyle\": \"--\"}),\n",
    "    \"hubert_kmeans\": (\"HuBERT (K-means)\", {\"color\": \"C2\", \"linestyle\": \"-.\"}),\n",
    "}\n",
    "metric_mapping = {\n",
    "    \"swuggy_all\": \"sWUGGY all\",\n",
    "    \"swuggy_invocab\": \"sWUGGY in-vocab\",\n",
    "    \"sblimp\": \"sBLIMP\",\n",
    "    \"tsc\": \"tSC\",\n",
    "    \"ssc\": \"sSC\",\n",
    "}\n",
    "hours = 6000\n",
    "\n",
    "slm = pl.read_csv(results / \"spoken-lm.csv\").filter(pl.col(\"model\").is_in(model_mapping), pl.col(\"hours\") == hours)\n",
    "fig, ax = plt.subplots(figsize=(TEXTWIDTH, 2), ncols=3, sharex=True, sharey=False)\n",
    "fig.set_layout_engine(\"tight\", pad=0.5, w_pad=1, rect=(0, 0.2, 1, 1))\n",
    "for j, metric in enumerate([\"swuggy_all\", \"sblimp\", \"tsc\"]):\n",
    "    for i, (model, (name, style)) in enumerate(model_mapping.items()):\n",
    "        metric_df = slm.filter(pl.col(\"model\") == model, pl.col(\"metric\") == metric).sort(\"layer\")\n",
    "        ax[j].plot(\n",
    "            metric_df[\"layer\"],\n",
    "            metric_df[\"score\"],\n",
    "            label=name,\n",
    "            zorder=len(styles) + 1 - i,\n",
    "            marker=\"o\",\n",
    "            markersize=4,\n",
    "            linewidth=1,\n",
    "            **style,\n",
    "        )\n",
    "    ax[j].set_xticks(range(5, 13))\n",
    "    ax[j].set_xticklabels(range(5, 13), fontsize=7)\n",
    "    ax[j].tick_params(axis=\"y\", labelsize=7)\n",
    "    ax[j].grid(linestyle=\"dotted\", alpha=0.4)\n",
    "    ax[j].set_xlabel(\"Layer\", fontsize=9)\n",
    "    ax[j].set_ylabel(rf\"{metric_mapping[metric]} (\\%)\", fontsize=9)\n",
    "leg = ax[1].legend(bbox_to_anchor=(0.5, -0.38), loc=\"upper center\", ncols=4, fontsize=9)\n",
    "leg.set_in_layout(False)\n",
    "plt.savefig(figures / \"slm-by-layer.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8665cd",
   "metadata": {},
   "source": [
    "Figure 13: Spoken language modeling against discriminability of the continuous representations. Dots are labeled by intermediate layer index. ABX for SpidR (Codebooks) is computed over codebook predictions\n",
    "\n",
    "Figure 14: Spoken language modeling against phonetic evaluation of the discrete units, with V = 256 units. Dots are labeled by intermediate layer index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e08f5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_mapping = {\n",
    "    \"swuggy_all\": \"sWUGGY all\",\n",
    "    \"swuggy_invocab\": \"sWUGGY in-vocab\",\n",
    "    \"sblimp\": \"sBLIMP\",\n",
    "    \"tsc\": \"tSC\",\n",
    "    \"ssc\": \"sSC\",\n",
    "    \"codebook_perplexity\": \"Codebook perplexity\",\n",
    "    \"phone_purity\": \"Phn. purity\",\n",
    "    \"cluster_purity\": \"Clus. purity\",\n",
    "    \"active_codewords\": \"Active units\",\n",
    "    \"pnmi\": \"PNMI\",\n",
    "    \"abx\": \"ABX\",\n",
    "    \"map\": \"MAP\",\n",
    "    \"abx_discrete\": \"ABX discrete\",\n",
    "}\n",
    "model_mapping = {\n",
    "    \"spidr_codebooks\": \"SpidR (Codebooks)\",\n",
    "    \"spidr\": \"SpidR (K-means)\",\n",
    "    \"spidr_kmeans\": \"SpidR (K-means)\",\n",
    "    \"hubert\": \"HuBERT (K-means)\",\n",
    "    \"hubert_kmeans\": \"HuBERT (K-means)\",\n",
    "}\n",
    "hours = 6000\n",
    "\n",
    "\n",
    "def average_scores(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    return (\n",
    "        df.group_by(\"model\", \"layer\", \"metric\")\n",
    "        .agg(pl.col(\"score\").mean())\n",
    "        .sort(\"model\", \"layer\")\n",
    "        .select(\"model\", \"layer\", \"metric\", \"score\")\n",
    "    )\n",
    "\n",
    "\n",
    "slm = pl.read_csv(results / \"spoken-lm.csv\").filter(pl.col(\"hours\") == hours).select(pl.exclude(\"hours\"))\n",
    "abx = pl.read_csv(results / \"abx-continuous.csv\").with_columns(pl.lit(\"abx\").alias(\"metric\"))\n",
    "speech_map = pl.read_csv(results / \"map-continuous.csv\").with_columns(pl.lit(\"map\").alias(\"metric\"))\n",
    "discrete = pl.read_csv(results / \"units-quality.csv\")\n",
    "abx_discrete = pl.read_csv(results / \"abx-discrete.csv\").with_columns(pl.lit(\"abx_discrete\").alias(\"metric\"))\n",
    "df = (\n",
    "    pl.concat([slm, average_scores(abx), average_scores(speech_map), discrete, average_scores(abx_discrete)])\n",
    "    .filter(pl.col(\"model\").is_in(model_mapping))\n",
    "    .with_columns(pl.col(\"model\").replace_strict(model_mapping), pl.col(\"metric\").replace_strict(metrics_mapping))\n",
    "    .pivot(on=\"metric\", values=\"score\", index=[\"model\", \"layer\"])\n",
    "    .drop_nulls()\n",
    "    .unpivot(\n",
    "        on=[\"sBLIMP\", \"sWUGGY all\", \"sWUGGY in-vocab\", \"tSC\", \"sSC\"],\n",
    "        variable_name=\"metric\",\n",
    "        value_name=\"slm\",\n",
    "        index=[\"model\", \"layer\", \"ABX\", \"MAP\", \"ABX discrete\", \"PNMI\"],\n",
    "    )\n",
    ")\n",
    "\n",
    "styles = [\"o\", \"s\", \"D\"]\n",
    "text_style = {\"size\": 3, \"ha\": \"center\", \"va\": \"center\", \"zorder\": 4, \"math_fontfamily\": \"dejavuserif\"}\n",
    "for metrics in [[\"ABX\", \"MAP\"], [\"ABX discrete\", \"PNMI\"]]:\n",
    "    fig, ax = plt.subplots(figsize=(6.5, 4), nrows=2, ncols=3, sharey=\"col\", sharex=\"row\")\n",
    "    fig.set_layout_engine(\"tight\", pad=0.1, w_pad=0, h_pad=2, rect=(0, 0.1, 1, 1))\n",
    "    for i, metric_y in enumerate([\"sWUGGY all\", \"sBLIMP\", \"tSC\"]):\n",
    "        selection = df.filter(pl.col(\"metric\") == metric_y)\n",
    "        for j, metric_x in enumerate(metrics):\n",
    "            coeff = pearsonr(selection[metric_x], selection[\"slm\"])[0]\n",
    "            line = linregress(selection[metric_x], selection[\"slm\"])\n",
    "            x = np.array([selection[metric_x].min(), selection[metric_x].max()])\n",
    "            ax[j, i].plot(\n",
    "                x,\n",
    "                line.intercept + line.slope * x,\n",
    "                color=\"gray\",\n",
    "                linewidth=0.5,\n",
    "                linestyle=\"dotted\",\n",
    "                zorder=0,\n",
    "            )\n",
    "            ax[j, i].text(\n",
    "                0.7,\n",
    "                0.6 if metric_x.startswith(\"ABX\") else 0.2,\n",
    "                rf\"$\\mathbf{{r = {coeff:.2f}}}$\",\n",
    "                transform=ax[j, i].transAxes,\n",
    "                fontsize=6,\n",
    "                ha=\"center\",\n",
    "                va=\"center\",\n",
    "            )\n",
    "            for k, model in enumerate([\"SpidR (Codebooks)\", \"SpidR (K-means)\", \"HuBERT (K-means)\"]):\n",
    "                subdf = selection.filter(pl.col(\"model\") == model)\n",
    "                scatter_abx = ax[j, i].scatter(\n",
    "                    subdf[metric_x],\n",
    "                    subdf[\"slm\"],\n",
    "                    label=model,\n",
    "                    s=25,\n",
    "                    zorder=3,\n",
    "                    alpha=0.7,\n",
    "                    edgecolor=f\"C{k}\",\n",
    "                    facecolor=\"none\",\n",
    "                    linewidth=1,\n",
    "                    marker=styles[k],\n",
    "                )\n",
    "                for idx, layer in enumerate(subdf[\"layer\"].cast(str).to_list()):\n",
    "                    ax[j, i].annotate(\n",
    "                        r\"$\\mathbf{\" + layer + \"}$\",\n",
    "                        (subdf[metric_x][idx], subdf[\"slm\"][idx]),\n",
    "                        color=f\"C{k}\",\n",
    "                        **text_style,\n",
    "                    )\n",
    "                ax[j, i].set_xlabel(metric_x + (r\" (\\%)\" if metric_x != \"PNMI\" else \"\"), fontsize=9)\n",
    "                ax[j, i].set_ylabel(metric_y + r\" (\\%)\", fontsize=9)\n",
    "                ax[j, i].grid(linestyle=\"dotted\", alpha=0.4)\n",
    "                ax[j, i].tick_params(axis=\"both\", labelsize=7)\n",
    "    leg = ax[1, 0].legend(bbox_to_anchor=(1.7, -0.32), loc=\"upper center\", ncols=3)\n",
    "    leg.set_in_layout(False)\n",
    "    plt.savefig(figures / f\"slm-{'-'.join(m.replace(' ', '-').lower() for m in metrics)}.pdf\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
